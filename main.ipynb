{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc83b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T19:34:38.370840208Z",
     "start_time": "2026-02-14T19:34:34.748293094Z"
    }
   },
   "outputs": [],
   "source": [
    "# prelude\n",
    "import energnn\n",
    "from ase.visualize import view\n",
    "from ase.build import molecule\n",
    "import torch_geometric.loader as tcg_loader\n",
    "import torch as tc\n",
    "from functools import reduce\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da2da9",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "This code only need to be executed at the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the files that needed.\n",
    "import energnn as erg\n",
    "erg.Toolbelt.download_environment_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52b1b1",
   "metadata": {},
   "source": [
    "# BasicGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset prepare.\n",
    "import pickle\n",
    "from energnn import DatasetAlexandria\n",
    "wbm = energnn.DatasetWBM()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecbc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(alex[60].y)\n",
    "#print(wbm[880].y)\n",
    "from energnn import DatasetAlexandria\n",
    "DatasetAlexandria(load_files=['005', '006', '007', '008', '009'], cutoff=8.0).dump(\"/home/aylwin/Projects/matbench/tmp/alexset005-009_cutoff8.dump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419d2a7",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new model\n",
    "model = energnn.ModelBasicGNNDeeper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun\n",
    "l1loss = tc.nn.L1Loss()\n",
    "mse_loss = tc.nn.MSELoss()\n",
    "def train_loss_fn(y, pred):\n",
    "    # This function will just train on e_above_hull aka y[:, 1]\n",
    "    return mse_loss(y[:, 1], pred[:, 1])\n",
    "def test_loss():\n",
    "    n = 0\n",
    "    energy_sum, e_above_hull_sum = 0, 0\n",
    "    num_Acc, n_Acc = 0, 0\n",
    "    def return_loss(y:tc.Tensor, pred:tc.Tensor)->dict:\n",
    "        nonlocal n, energy_sum, e_above_hull_sum, num_Acc, n_Acc\n",
    "        energy = l1loss(y[:, 0], pred[:, 0])\n",
    "        e_above_hull = l1loss(y[:, 1], pred[:, 1])\n",
    "        n+=1\n",
    "        # determine the Acc.  *----XOR---*\n",
    "        num_Acc += (lambda x, y:(1-(x+y)%2).sum())((y[:, 1]>0).int(), (pred[:, 1]>0).int())\n",
    "        #print((y[:, 1]>0).int().sum())\n",
    "        n_Acc += y.shape[0]\n",
    "        # add energy and e_above_hull_sum\n",
    "        energy_sum += float(energy)\n",
    "        e_above_hull_sum += float(e_above_hull)\n",
    "        return {\"avr_energy\":energy_sum/n, \"avr_e_above_hull\":e_above_hull_sum/n, \"avr_accuracy\":num_Acc/n_Acc}\n",
    "    return return_loss\n",
    "for epoch in range(40):\n",
    "    print(f\"Epoch {epoch} ================>\")\n",
    "    print(\"train:\")\n",
    "    for train_name in ('000-004', '005-009', '010-014'):\n",
    "        with open(f\"/home/aylwin/Projects/matbench/tmp/alexset{train_name}_cutoff8.dump\", 'rb') as f:\n",
    "            train_set = pickle.load(f)\n",
    "        model, log = mb.tcg_trainer(\n",
    "            model=model,\n",
    "            dataset=train_set,\n",
    "            optimizer=tc.optim.Adam(model.parameters(), lr=0.0001),\n",
    "            device='cuda',\n",
    "            epoch=1, \n",
    "            batch_size=64,\n",
    "            num_workers=4,\n",
    "            loss_fn = train_loss_fn\n",
    "        )\n",
    "    print(\"test:\")\n",
    "    mb.tcg_tester(\n",
    "        model=model,\n",
    "        dataset=wbm,\n",
    "        device='cuda',\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        loss_fn=test_loss,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "l1loss = tc.nn.L1Loss()\n",
    "def test_loss():\n",
    "    n = 0\n",
    "    energy_sum, e_above_hull_sum = 0, 0\n",
    "    num_Acc, n_Acc = 0, 0\n",
    "    def return_loss(y:tc.Tensor, pred:tc.Tensor)->dict:\n",
    "        nonlocal n, energy_sum, e_above_hull_sum, num_Acc, n_Acc\n",
    "        energy = l1loss(y[:, 0], pred[:, 0])\n",
    "        e_above_hull = l1loss(y[:, 1], pred[:, 1])\n",
    "        n+=1\n",
    "        # determine the Acc.  *----XOR---*\n",
    "        num_Acc += (lambda x, y:(1-(x+y)%2).sum())((y[:, 1]>0).int(), (pred[:, 1]>0).int())\n",
    "        #print((y[:, 1]>0).int().sum())\n",
    "        n_Acc += y.shape[0]\n",
    "        # add energy and e_above_hull_sum\n",
    "        energy_sum += float(energy)\n",
    "        e_above_hull_sum += float(e_above_hull)\n",
    "        return {\"avr_energy\":energy_sum/n, \"avr_e_above_hull\":e_above_hull_sum/n, \"avr_accuracy\":num_Acc/n_Acc}\n",
    "    return return_loss\n",
    "energnn.tcg_tester(\n",
    "    model=model,\n",
    "    dataset=wbm,\n",
    "    device='cuda',\n",
    "    batch_size=256,\n",
    "    num_workers=32,\n",
    "    loss_fn=test_loss,\n",
    ")\n",
    "energnn.tcg_tester(\n",
    "    model=model,\n",
    "    dataset=alex_train,\n",
    "    device='cuda',\n",
    "    batch_size=256,\n",
    "    num_workers=32,\n",
    "    loss_fn=test_loss,\n",
    ")\n",
    "energnn.tcg_tester(\n",
    "    model=model,\n",
    "    dataset=alex_test,\n",
    "    device='cuda',\n",
    "    batch_size=256,\n",
    "    num_workers=32,\n",
    "    loss_fn=test_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea495b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the module.\n",
    "import pickle\n",
    "with open(\"/home/aylwin/Projects/matbench/tmp/good_model.ptm\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fffdc",
   "metadata": {},
   "source": [
    "# EnerGNN\n",
    "---\n",
    "This model `.forward` will return the total energy for the structure input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072750c",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cac68c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer start...\n",
      "Epoch 0 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]/home/aylwin/Projects/matbench/.pixi/envs/default/lib/python3.12/site-packages/torch/nn/modules/loss.py:129: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/aylwin/Projects/matbench/EnerGNN/energnn.py:487: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1769243867349/work/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
      "  sum_loss += float(loss)\n",
      "100%|█████████▉| 1562/1563 [01:51<00:00, 14.01it/s, avr_loss:804182.429456176]  /home/aylwin/Projects/matbench/.pixi/envs/default/lib/python3.12/site-packages/torch/nn/modules/loss.py:129: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 1563/1563 [01:51<00:00, 14.04it/s, avr_loss:803782.7304450076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:51<00:00, 14.03it/s, avr_loss:35876.97659280281] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:51<00:00, 14.02it/s, avr_loss:3240.853769943032] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:51<00:00, 13.99it/s, avr_loss:303.6902897859794] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:51<00:00, 13.98it/s, avr_loss:168.96488717604507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:51<00:00, 13.97it/s, avr_loss:109.02095679968347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:51<00:00, 13.98it/s, avr_loss:123.70798621784779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:51<00:00, 13.98it/s, avr_loss:124.04268704571177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:51<00:00, 13.97it/s, avr_loss:131.60589655621723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 ===========>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:52<00:00, 13.94it/s, avr_loss:102.52171038666064]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(EnerG(\n",
       "   (conv1): NNConv(4, 8, aggr=add, nn=Sequential(\n",
       "     (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.1)\n",
       "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (3): LeakyReLU(negative_slope=0.1)\n",
       "   ))\n",
       "   (conv2): NNConv(8, 64, aggr=add, nn=Sequential(\n",
       "     (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.1)\n",
       "     (2): Linear(in_features=64, out_features=512, bias=True)\n",
       "     (3): LeakyReLU(negative_slope=0.1)\n",
       "   ))\n",
       "   (conv3): NNConv(64, 128, aggr=add, nn=Sequential(\n",
       "     (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.1)\n",
       "     (2): Linear(in_features=64, out_features=8192, bias=True)\n",
       "     (3): LeakyReLU(negative_slope=0.1)\n",
       "   ))\n",
       "   (fc1): Linear(128, 128, bias=True)\n",
       "   (fc2): Linear(128, 64, bias=True)\n",
       "   (fc3): Linear(64, 1, bias=True)\n",
       "   (leaky): LeakyReLU(negative_slope=0.1)\n",
       " ),\n",
       " {'avr_loss': [803782.7304450076,\n",
       "   35876.97659280281,\n",
       "   3240.853769943032,\n",
       "   303.6902897859794,\n",
       "   168.96488717604507,\n",
       "   109.02095679968347,\n",
       "   123.70798621784779,\n",
       "   124.04268704571177,\n",
       "   131.60589655621723,\n",
       "   102.52171038666064]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is an example of how to train the EnerGNN model on the Alexandria dataset.\n",
    "import energnn as erg\n",
    "import torch as tc\n",
    "\n",
    "# Load the dataset.\n",
    "# dataset = erg.datasetAlexandriaNeo(load_files=['000'], cutoff=6.0) # This just run in the first time.\n",
    "dataset = erg.Toolbelt.pk_load(\"/Alexandria/000_cutoff6.dump\")\n",
    "\n",
    "# init model\n",
    "model = erg.EnerG()\n",
    "l1loss = tc.nn.L1Loss()\n",
    "model, log = erg.tcg_trainer(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    optimizer=tc.optim.Adam(model.parameters(), lr=0.0001),\n",
    "    device='cuda',\n",
    "    epoch=10, \n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    loss_fn = l1loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492af114",
   "metadata": {},
   "source": [
    "## Get force from structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "118bf191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init the Dataset WBM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256963/256963 [00:38<00:00, 6668.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 104], y=[1, 2], node_features=[8, 4], edge_weight=[104])\n",
      "Data(edge_index=[2, 104], y=[1, 2], node_features=[8, 4], edge_weight=[104])\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import energnn as erg\n",
    "dataset = erg.DatasetWBM()\n",
    "print(dataset[0])\n",
    "for i in dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b5a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
